{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39cf61e-7ea9-4d44-9fef-84f5a98c8790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "하위 디렉터리 또는 파일 results이(가) 이미 있습니다.\n"
     ]
    }
   ],
   "source": [
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1761714-d74d-4ee0-9ca8-0c0bef4eaba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e9444-a5f4-4a63-b910-cd4bae777b34",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88054b-712f-44ca-9e27-cdac1c60c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d2aad4-6b20-4c2a-930b-75800bf31c12",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaceb85e-46fe-4012-965f-9245a0e49567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, use_bn, use_xavier):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layer = n_layer\n",
    "        self.act = act\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn\n",
    "        self.use_xavier = use_xavier\n",
    "        \n",
    "        # ====== Create Linear Layers ====== #\n",
    "        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n",
    "        \n",
    "        self.linears = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        for i in range(self.n_layer-1):\n",
    "            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
    "            if self.use_bn:\n",
    "                self.bns.append(nn.BatchNorm1d(self.hid_dim))\n",
    "                \n",
    "        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n",
    "        \n",
    "        # ====== Create Activation Function ====== #\n",
    "        if self.act == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif self.act == 'tanh':\n",
    "            self.act == nn.Tanh()\n",
    "        elif self.act == 'sigmoid':\n",
    "            self.act = nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError('no valid activation function selected!')\n",
    "        \n",
    "        # ====== Create Regularization Layer ======= #\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        if self.use_xavier:\n",
    "            self.xavier_init()\n",
    "          \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        for i in range(len(self.linears)):\n",
    "            x = self.act(self.linears[i](x))\n",
    "            if self.use_bn:\n",
    "                x = self.bns[i](x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def xavier_init(self):\n",
    "        for linear in self.linears:\n",
    "            nn.init.xavier_normal_(linear.weight)\n",
    "            linear.bias.data.fill_(0.01)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235728f-2acc-404f-8601-6b57ee476738",
   "metadata": {},
   "source": [
    "## Train, Validate, Test and Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d54d1f-584e-4d99-b093-f5018137bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, partition, optimizer, criterion, args):\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
    "                                              batch_size=args.train_batch_size, \n",
    "                                              shuffle=True, num_workers=2)\n",
    "    net.train()\n",
    "   \n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 3072)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = 100 * correct / total\n",
    "    return net, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56320bf6-30da-4605-bb2d-364f3ca833d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, partition, criterion, args):\n",
    "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
    "                                            batch_size=args.test_batch_size, \n",
    "                                            shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 3072)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "853e4564-96b8-4ac3-9c82-c54571522a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, partition, args):\n",
    "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
    "                                             batch_size=args.test_batch_size, \n",
    "                                             shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 3072)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b04557f-4d5a-4d54-8941-9fbd16c10fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "  \n",
    "    net = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act, args.dropout, args.use_bn, args.use_xavier)\n",
    "    \n",
    "    device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2, momentum = 0.9)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2, momentum = 0.9)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "        \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
    "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    test_acc = test(net, partition, args)    \n",
    "    \n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcc54b-c3cb-48bd-a5f4-cf29c6d1d507",
   "metadata": {},
   "source": [
    "## Manage Experiment Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57fa1576-d34a-4569-a17e-83e99dfd6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "def save_exp_result(setting, result):\n",
    "    exp_name = setting['exp_name']\n",
    "    del setting['epoch']\n",
    "    del setting['test_batch_size']\n",
    "\n",
    "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
    "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
    "    result.update(setting)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "    \n",
    "def load_exp_result(exp_name):\n",
    "    dir_path = './results'\n",
    "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
    "    list_result = []\n",
    "    for filename in filenames:\n",
    "        if exp_name in filename:\n",
    "            with open(join(dir_path, filename), 'r') as infile:\n",
    "                results = json.load(infile)\n",
    "                list_result.append(results)\n",
    "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bcaea2-12e8-48b0-92f2-60aa05262359",
   "metadata": {},
   "source": [
    "## Visualization Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cf1ce00-da81-496e-b458-291cb8df4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(var1, var2, df):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(15, 6)\n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
    "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
    "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
    "    \n",
    "    ax[0].set_title('Train Accuracy')\n",
    "    ax[1].set_title('Validation Accuracy')\n",
    "    ax[2].set_title('Test Accuracy')\n",
    "\n",
    "    \n",
    "def plot_loss_variation(var1, var2, df, **kwargs):\n",
    "\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_losses = list(row.train_losses)[0]\n",
    "            val_losses = list(row.val_losses)[0]\n",
    "\n",
    "            for epoch, train_loss in enumerate(train_losses):\n",
    "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_loss in enumerate(val_losses):\n",
    "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train loss vs Val loss')\n",
    "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
    "\n",
    "\n",
    "def plot_acc_variation(var1, var2, df, **kwargs):\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_accs = list(row.train_accs)[0]\n",
    "            val_accs = list(row.val_accs)[0]\n",
    "            test_acc = list(row.test_acc)[0]\n",
    "\n",
    "            for epoch, train_acc in enumerate(train_accs):\n",
    "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_acc in enumerate(val_accs):\n",
    "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
    "\n",
    "    def show_acc(x, y, metric, **kwargs):\n",
    "        plt.scatter(x, y, alpha=0.3, s=1)\n",
    "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
    "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
    "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
    "\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
    "    plt.subplots_adjust(top=0.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47322ef6-1f12-4bef-a7f4-c873252aff64",
   "metadata": {},
   "source": [
    "## Experiment1. N_layer vs Hidden_Dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5753e980-25bd-4c04-b17b-a85795ba6302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(exp_name='exp1_n_layer_hid_dim', in_dim=3072, out_dim=10, hid_dim=500, act='relu', dropout=0.2, use_bn=True, l2=1e-05, use_xavier=True, optim='RMSprop', lr=0.0015, epoch=10, train_batch_size=256, test_batch_size=1024, n_layer=1)\n",
      "Epoch 0, Acc(train/val): 37.78/41.36, Loss(train/val) 2.56/1.68. Took 12.04 sec\n",
      "Epoch 1, Acc(train/val): 46.30/41.17, Loss(train/val) 1.57/1.75. Took 12.03 sec\n",
      "Epoch 2, Acc(train/val): 49.01/39.64, Loss(train/val) 1.49/1.82. Took 12.24 sec\n",
      "Epoch 3, Acc(train/val): 51.10/35.36, Loss(train/val) 1.44/2.09. Took 11.96 sec\n",
      "Epoch 4, Acc(train/val): 53.22/39.57, Loss(train/val) 1.37/1.96. Took 12.13 sec\n",
      "Epoch 5, Acc(train/val): 54.97/39.34, Loss(train/val) 1.33/2.23. Took 12.19 sec\n",
      "Epoch 6, Acc(train/val): 56.92/42.51, Loss(train/val) 1.28/1.90. Took 12.49 sec\n",
      "Epoch 7, Acc(train/val): 58.51/43.18, Loss(train/val) 1.23/1.81. Took 12.23 sec\n",
      "Epoch 8, Acc(train/val): 59.60/36.97, Loss(train/val) 1.20/3.17. Took 12.29 sec\n",
      "Epoch 9, Acc(train/val): 61.65/46.68, Loss(train/val) 1.16/1.77. Took 12.13 sec\n",
      "Namespace(exp_name='exp1_n_layer_hid_dim', in_dim=3072, out_dim=10, hid_dim=300, act='relu', dropout=0.2, use_bn=True, l2=1e-05, use_xavier=True, optim='RMSprop', lr=0.0015, epoch=10, train_batch_size=256, test_batch_size=1024, n_layer=1)\n",
      "Epoch 0, Acc(train/val): 38.48/38.01, Loss(train/val) 2.17/1.80. Took 12.00 sec\n",
      "Epoch 1, Acc(train/val): 47.02/41.30, Loss(train/val) 1.53/1.75. Took 11.82 sec\n",
      "Epoch 2, Acc(train/val): 49.75/39.63, Loss(train/val) 1.45/1.86. Took 11.86 sec\n",
      "Epoch 3, Acc(train/val): 51.95/43.02, Loss(train/val) 1.40/1.74. Took 11.97 sec\n",
      "Epoch 4, Acc(train/val): 54.07/40.37, Loss(train/val) 1.34/2.01. Took 11.84 sec\n",
      "Epoch 5, Acc(train/val): 55.88/38.50, Loss(train/val) 1.29/2.04. Took 12.29 sec\n",
      "Epoch 6, Acc(train/val): 57.50/43.66, Loss(train/val) 1.24/1.82. Took 12.19 sec\n",
      "Epoch 7, Acc(train/val): 59.42/45.32, Loss(train/val) 1.19/1.80. Took 12.09 sec\n",
      "Epoch 8, Acc(train/val): 60.67/47.33, Loss(train/val) 1.16/1.73. Took 11.99 sec\n",
      "Epoch 9, Acc(train/val): 61.97/44.40, Loss(train/val) 1.12/1.93. Took 12.00 sec\n",
      "Namespace(exp_name='exp1_n_layer_hid_dim', in_dim=3072, out_dim=10, hid_dim=500, act='relu', dropout=0.2, use_bn=True, l2=1e-05, use_xavier=True, optim='RMSprop', lr=0.0015, epoch=10, train_batch_size=256, test_batch_size=1024, n_layer=2)\n",
      "Epoch 0, Acc(train/val): 36.91/40.84, Loss(train/val) 1.80/1.67. Took 12.13 sec\n",
      "Epoch 1, Acc(train/val): 45.60/42.32, Loss(train/val) 1.53/1.66. Took 12.07 sec\n",
      "Epoch 2, Acc(train/val): 49.75/44.24, Loss(train/val) 1.42/1.61. Took 12.10 sec\n",
      "Epoch 3, Acc(train/val): 52.69/49.10, Loss(train/val) 1.34/1.44. Took 12.33 sec\n",
      "Epoch 4, Acc(train/val): 55.08/48.07, Loss(train/val) 1.27/1.48. Took 12.27 sec\n",
      "Epoch 5, Acc(train/val): 57.25/45.67, Loss(train/val) 1.21/1.59. Took 12.31 sec\n",
      "Epoch 6, Acc(train/val): 59.46/50.74, Loss(train/val) 1.15/1.47. Took 11.95 sec\n",
      "Epoch 7, Acc(train/val): 61.70/48.21, Loss(train/val) 1.08/1.61. Took 11.99 sec\n",
      "Epoch 8, Acc(train/val): 63.80/49.34, Loss(train/val) 1.02/1.54. Took 11.92 sec\n",
      "Epoch 9, Acc(train/val): 65.77/51.49, Loss(train/val) 0.97/1.49. Took 11.86 sec\n",
      "Namespace(exp_name='exp1_n_layer_hid_dim', in_dim=3072, out_dim=10, hid_dim=300, act='relu', dropout=0.2, use_bn=True, l2=1e-05, use_xavier=True, optim='RMSprop', lr=0.0015, epoch=10, train_batch_size=256, test_batch_size=1024, n_layer=2)\n",
      "Epoch 0, Acc(train/val): 36.86/35.98, Loss(train/val) 1.77/1.89. Took 12.18 sec\n",
      "Epoch 1, Acc(train/val): 45.86/40.92, Loss(train/val) 1.53/1.66. Took 12.01 sec\n",
      "Epoch 2, Acc(train/val): 49.70/42.71, Loss(train/val) 1.43/1.66. Took 12.05 sec\n",
      "Epoch 3, Acc(train/val): 52.39/44.62, Loss(train/val) 1.35/1.58. Took 12.09 sec\n",
      "Epoch 4, Acc(train/val): 55.10/43.35, Loss(train/val) 1.28/1.67. Took 12.25 sec\n",
      "Epoch 5, Acc(train/val): 57.00/49.20, Loss(train/val) 1.21/1.49. Took 12.16 sec\n",
      "Epoch 6, Acc(train/val): 59.49/47.72, Loss(train/val) 1.15/1.55. Took 12.27 sec\n",
      "Epoch 7, Acc(train/val): 60.99/47.56, Loss(train/val) 1.11/1.59. Took 11.97 sec\n",
      "Epoch 8, Acc(train/val): 63.15/49.66, Loss(train/val) 1.04/1.51. Took 11.90 sec\n",
      "Epoch 9, Acc(train/val): 64.76/51.77, Loss(train/val) 1.00/1.46. Took 11.88 sec\n",
      "Namespace(exp_name='exp1_n_layer_hid_dim', in_dim=3072, out_dim=10, hid_dim=500, act='relu', dropout=0.2, use_bn=True, l2=1e-05, use_xavier=True, optim='RMSprop', lr=0.0015, epoch=10, train_batch_size=256, test_batch_size=1024, n_layer=3)\n",
      "Epoch 0, Acc(train/val): 35.33/39.72, Loss(train/val) 1.84/1.72. Took 11.88 sec\n",
      "Epoch 1, Acc(train/val): 45.23/39.60, Loss(train/val) 1.52/1.73. Took 11.87 sec\n",
      "Epoch 2, Acc(train/val): 49.57/43.63, Loss(train/val) 1.40/1.67. Took 11.91 sec\n",
      "Epoch 3, Acc(train/val): 52.55/46.53, Loss(train/val) 1.33/1.53. Took 11.85 sec\n",
      "Epoch 4, Acc(train/val): 55.39/47.53, Loss(train/val) 1.24/1.51. Took 11.99 sec\n",
      "Epoch 5, Acc(train/val): 57.57/48.31, Loss(train/val) 1.18/1.51. Took 11.91 sec\n",
      "Epoch 6, Acc(train/val): 59.41/49.79, Loss(train/val) 1.13/1.47. Took 11.86 sec\n",
      "Epoch 7, Acc(train/val): 62.05/49.85, Loss(train/val) 1.06/1.51. Took 12.02 sec\n",
      "Epoch 8, Acc(train/val): 64.05/47.10, Loss(train/val) 1.00/1.59. Took 12.01 sec\n",
      "Epoch 9, Acc(train/val): 66.13/47.33, Loss(train/val) 0.94/1.71. Took 12.11 sec\n",
      "Namespace(exp_name='exp1_n_layer_hid_dim', in_dim=3072, out_dim=10, hid_dim=300, act='relu', dropout=0.2, use_bn=True, l2=1e-05, use_xavier=True, optim='RMSprop', lr=0.0015, epoch=10, train_batch_size=256, test_batch_size=1024, n_layer=3)\n",
      "Epoch 0, Acc(train/val): 36.66/38.72, Loss(train/val) 1.78/1.69. Took 12.18 sec\n",
      "Epoch 1, Acc(train/val): 45.99/41.01, Loss(train/val) 1.51/1.66. Took 12.06 sec\n",
      "Epoch 2, Acc(train/val): 49.55/43.98, Loss(train/val) 1.41/1.61. Took 12.02 sec\n",
      "Epoch 3, Acc(train/val): 52.78/48.61, Loss(train/val) 1.32/1.45. Took 12.10 sec\n",
      "Epoch 4, Acc(train/val): 55.05/45.29, Loss(train/val) 1.26/1.61. Took 11.92 sec\n",
      "Epoch 5, Acc(train/val): 57.01/48.21, Loss(train/val) 1.20/1.53. Took 12.39 sec\n",
      "Epoch 6, Acc(train/val): 58.92/48.81, Loss(train/val) 1.14/1.51. Took 12.86 sec\n",
      "Epoch 7, Acc(train/val): 60.95/43.25, Loss(train/val) 1.09/1.84. Took 12.25 sec\n",
      "Epoch 8, Acc(train/val): 62.21/49.54, Loss(train/val) 1.06/1.51. Took 12.09 sec\n",
      "Epoch 9, Acc(train/val): 64.35/50.59, Loss(train/val) 0.99/1.48. Took 12.07 sec\n"
     ]
    }
   ],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp1_n_layer_hid_dim\"\n",
    "\n",
    "# ====== Model Capacity ====== #\n",
    "args.in_dim = 3072\n",
    "args.out_dim = 10\n",
    "args.hid_dim = 100\n",
    "args.act = 'relu'\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.dropout = 0.2\n",
    "args.use_bn = True\n",
    "args.l2 = 0.00001\n",
    "args.use_xavier = True\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 0.0015\n",
    "args.epoch = 10\n",
    "\n",
    "args.train_batch_size = 256\n",
    "args.test_batch_size = 1024\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'n_layer'\n",
    "name_var2 = 'hid_dim'\n",
    "list_var1 = [1, 2, 3]\n",
    "list_var2 = [500, 300]\n",
    "\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        setattr(args, name_var1, var1)\n",
    "        setattr(args, name_var2, var2)\n",
    "        print(args)\n",
    "                \n",
    "        setting, result = experiment(partition, deepcopy(args))\n",
    "        save_exp_result(setting, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d35ad251-3587-4e42-a39b-3a600b273d60",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m var2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhid_dim\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m load_exp_result(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m plot_acc(var1, var2, df)\n\u001b[0;32m      6\u001b[0m plot_loss_variation(var1, var2, df, sharey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m#sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m plot_acc_variation(var1, var2, df, margin_titles\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sharey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[38], line 7\u001b[0m, in \u001b[0;36mplot_acc\u001b[1;34m(var1, var2, df)\u001b[0m\n\u001b[0;32m      4\u001b[0m fig\u001b[38;5;241m.\u001b[39mset_size_inches(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      5\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_style(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarkgrid\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxes.facecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.9\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m----> 7\u001b[0m sns\u001b[38;5;241m.\u001b[39mbarplot(x\u001b[38;5;241m=\u001b[39mvar1, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, hue\u001b[38;5;241m=\u001b[39mvar2, data\u001b[38;5;241m=\u001b[39mdf, ax\u001b[38;5;241m=\u001b[39max[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      8\u001b[0m sns\u001b[38;5;241m.\u001b[39mbarplot(x\u001b[38;5;241m=\u001b[39mvar1, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, hue\u001b[38;5;241m=\u001b[39mvar2, data\u001b[38;5;241m=\u001b[39mdf, ax\u001b[38;5;241m=\u001b[39max[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      9\u001b[0m sns\u001b[38;5;241m.\u001b[39mbarplot(x\u001b[38;5;241m=\u001b[39mvar1, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, hue\u001b[38;5;241m=\u001b[39mvar2, data\u001b[38;5;241m=\u001b[39mdf, ax\u001b[38;5;241m=\u001b[39max[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:2763\u001b[0m, in \u001b[0;36mbarplot\u001b[1;34m(data, x, y, hue, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge, ci, ax, **kwargs)\u001b[0m\n\u001b[0;32m   2760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2761\u001b[0m     ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mgca()\n\u001b[1;32m-> 2763\u001b[0m plotter\u001b[38;5;241m.\u001b[39mplot(ax, kwargs)\n\u001b[0;32m   2764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ax\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:1587\u001b[0m, in \u001b[0;36m_BarPlotter.plot\u001b[1;34m(self, ax, bar_kws)\u001b[0m\n\u001b[0;32m   1585\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make the plot.\"\"\"\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_bars(ax, bar_kws)\n\u001b[1;32m-> 1587\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotate_axes(ax)\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1589\u001b[0m     ax\u001b[38;5;241m.\u001b[39minvert_yaxis()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:767\u001b[0m, in \u001b[0;36m_CategoricalPlotter.annotate_axes\u001b[1;34m(self, ax)\u001b[0m\n\u001b[0;32m    764\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_data) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m.5\u001b[39m, auto\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhue_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 767\u001b[0m     ax\u001b[38;5;241m.\u001b[39mlegend(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhue_title)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:322\u001b[0m, in \u001b[0;36mAxes.legend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;129m@_docstring\u001b[39m\u001b[38;5;241m.\u001b[39mdedent_interpd\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlegend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    206\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    Place a legend on the Axes.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    .. plot:: gallery/text_labels_and_annotations/legend.py\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     handles, labels, kwargs \u001b[38;5;241m=\u001b[39m mlegend\u001b[38;5;241m.\u001b[39m_parse_legend_args([\u001b[38;5;28mself\u001b[39m], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend_ \u001b[38;5;241m=\u001b[39m mlegend\u001b[38;5;241m.\u001b[39mLegend(\u001b[38;5;28mself\u001b[39m, handles, labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend_\u001b[38;5;241m.\u001b[39m_remove_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_legend\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\legend.py:1361\u001b[0m, in \u001b[0;36m_parse_legend_args\u001b[1;34m(axs, handles, labels, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     handles \u001b[38;5;241m=\u001b[39m [handle \u001b[38;5;28;01mfor\u001b[39;00m handle, label\n\u001b[0;32m   1358\u001b[0m                \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(_get_legend_handles(axs, handlers), labels)]\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# 0 args: automatically detect labels and handles.\u001b[39;00m\n\u001b[1;32m-> 1361\u001b[0m     handles, labels \u001b[38;5;241m=\u001b[39m _get_legend_handles_labels(axs, handlers)\n\u001b[0;32m   1362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handles:\n\u001b[0;32m   1363\u001b[0m         log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1364\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo artists with labels found to put in legend.  Note that \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1365\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martists whose label start with an underscore are ignored \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen legend() is called with no argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\legend.py:1291\u001b[0m, in \u001b[0;36m_get_legend_handles_labels\u001b[1;34m(axs, legend_handler_map)\u001b[0m\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m _get_legend_handles(axs, legend_handler_map):\n\u001b[0;32m   1290\u001b[0m     label \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mget_label()\n\u001b[1;32m-> 1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m label\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1292\u001b[0m         handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m   1293\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'startswith'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var1 = 'n_layer'\n",
    "var2 = 'hid_dim'\n",
    "df = load_exp_result('exp1')\n",
    "\n",
    "plot_acc(var1, var2, df)\n",
    "plot_loss_variation(var1, var2, df, sharey=False) #sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\n",
    "plot_acc_variation(var1, var2, df, margin_titles=True, sharey=True) #margin_titles를 True로 하면 그래프의 가장자리에 var1과 var2 값이 표시되고 False로 하면 각 subplot 위에 표시됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a00a86-3f15-4ce8-89c4-f1b45054caa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308ddb3-0bf5-44cc-865c-56c106ac9dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
